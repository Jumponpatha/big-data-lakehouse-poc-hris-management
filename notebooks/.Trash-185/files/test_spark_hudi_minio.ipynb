{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb0f4179-799a-47a7-9c68-2af1549d4e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7eed2045-efd1-4daf-9f22-035430358a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/28 14:46:50 INFO SparkContext: Running Spark version 3.5.6\n",
      "25/10/28 14:46:50 INFO SparkContext: OS info Linux, 6.6.87.1-microsoft-standard-WSL2, amd64\n",
      "25/10/28 14:46:50 INFO SparkContext: Java version 11.0.27\n",
      "25/10/28 14:46:50 INFO ResourceUtils: ==============================================================\n",
      "25/10/28 14:46:50 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/10/28 14:46:50 INFO ResourceUtils: ==============================================================\n",
      "25/10/28 14:46:50 INFO SparkContext: Submitted application: TestHuditoMinIO\n",
      "25/10/28 14:46:50 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/10/28 14:46:50 INFO ResourceProfile: Limiting resource is cpu\n",
      "25/10/28 14:46:50 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/10/28 14:46:50 INFO SecurityManager: Changing view acls to: spark\n",
      "25/10/28 14:46:50 INFO SecurityManager: Changing modify acls to: spark\n",
      "25/10/28 14:46:50 INFO SecurityManager: Changing view acls groups to: \n",
      "25/10/28 14:46:50 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/10/28 14:46:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY\n",
      "25/10/28 14:46:50 INFO Utils: Successfully started service 'sparkDriver' on port 45213.\n",
      "25/10/28 14:46:50 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/10/28 14:46:50 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/10/28 14:46:50 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/10/28 14:46:50 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/10/28 14:46:50 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/10/28 14:46:50 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e6a0b852-6b28-443e-9153-0a13889a6a98\n",
      "25/10/28 14:46:50 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "25/10/28 14:46:50 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/10/28 14:46:50 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "25/10/28 14:46:50 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "25/10/28 14:46:50 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...\n",
      "25/10/28 14:46:50 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.6:7077 after 8 ms (0 ms spent in bootstraps)\n",
      "25/10/28 14:46:50 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20251028144650-0001\n",
      "25/10/28 14:46:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20251028144650-0001/0 on worker-20251028143115-172.18.0.9-35981 (172.18.0.9:35981) with 2 core(s)\n",
      "25/10/28 14:46:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20251028144650-0001/0 on hostPort 172.18.0.9:35981 with 2 core(s), 1024.0 MiB RAM\n",
      "25/10/28 14:46:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44265.\n",
      "25/10/28 14:46:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20251028144650-0001/1 on worker-20251028143115-172.18.0.7-45801 (172.18.0.7:45801) with 2 core(s)\n",
      "25/10/28 14:46:50 INFO NettyBlockTransferService: Server created on 9361fa1ccb18:44265\n",
      "25/10/28 14:46:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/10/28 14:46:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20251028144650-0001/1 on hostPort 172.18.0.7:45801 with 2 core(s), 1024.0 MiB RAM\n",
      "25/10/28 14:46:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 9361fa1ccb18, 44265, None)\n",
      "25/10/28 14:46:50 INFO BlockManagerMasterEndpoint: Registering block manager 9361fa1ccb18:44265 with 434.4 MiB RAM, BlockManagerId(driver, 9361fa1ccb18, 44265, None)\n",
      "25/10/28 14:46:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 9361fa1ccb18, 44265, None)\n",
      "25/10/28 14:46:50 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 9361fa1ccb18, 44265, None)\n",
      "25/10/28 14:46:50 INFO SingleEventLogFileWriter: Logging events to file:/opt/spark/events/app-20251028144650-0001.inprogress\n",
      "25/10/28 14:46:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20251028144650-0001/1 is now RUNNING\n",
      "25/10/28 14:46:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20251028144650-0001/0 is now RUNNING\n",
      "25/10/28 14:46:50 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TestHuditoMinIO\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .config(\"spark.hadoop.hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61dd8a61-35af-4075-876d-7c1ca01907d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://9361fa1ccb18:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.6</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>TestHuditoMinIO</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://spark-master:7077 appName=TestHuditoMinIO>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show Spark Context\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "831d9a14-edcc-485c-8117-e2552e2472a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:===================>                                       (1 + 2) / 3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+--------------------+\n",
      "| id| name|age|      curr_timestamp|\n",
      "+---+-----+---+--------------------+\n",
      "|  1|Alice| 24|2025-10-28 14:46:...|\n",
      "|  2|  Bob| 30|2025-10-28 14:46:...|\n",
      "+---+-----+---+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "data = [(1, \"Alice\", 24), (2, \"Bob\", 30)]\n",
    "columns = [\"id\", \"name\", \"age\"]\n",
    "df = spark.createDataFrame(data, columns).withColumn(\"curr_timestamp\", current_timestamp())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d0bb0a3-a76c-4186-b555-9abd697c93fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/27 16:12:07 WARN HoodieSparkSqlWriterInternal: hoodie table at s3a://warehouse/customer_hudi already exists. Deleting existing data & overwriting with new data.\n",
      "25/10/27 16:12:13 WARN HoodieBackedTableMetadataWriter: Partition stats index cannot be enabled for a non-partitioned table. Removing from initialization list. Please disable hoodie.metadata.index.partition.stats.enable\n",
      "25/10/27 16:12:21 WARN HoodieBackedTableMetadataWriter: Partition stats index cannot be enabled for a non-partitioned table. Removing from initialization list. Please disable hoodie.metadata.index.partition.stats.enable\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Define Hudi options\n",
    "hudi_options = {\n",
    "    \"hoodie.table.name\": \"customer_hudi\",\n",
    "    \"hoodie.datasource.write.recordkey.field\": \"id\",\n",
    "    \"hoodie.datasource.write.precombine.field\": \"curr_timestamp\",\n",
    "    \"hoodie.datasource.write.table.type\": \"COPY_ON_WRITE\",\n",
    "    \"hoodie.datasource.hive_sync.enable\": \"true\",\n",
    "    \"hoodie.datasource.hive_sync.mode\": \"hms\",\n",
    "    \"hoodie.datasource.hive_sync.database\": \"default\",\n",
    "    \"hoodie.datasource.hive_sync.table\": \"customer_hudi\",\n",
    "    \"hoodie.datasource.hive_sync.metastore.uris\": \"thrift://hive-metastore:9083\",\n",
    "    \"hoodie.datasource.write.hive_style_partitioning\": \"true\",\n",
    "    \"hoodie.enable.data.skipping\": \"true\",\n",
    "    \"hoodie.metadata.enable\": \"true\",\n",
    "    \"hoodie.metadata.index.column.stats.enable\": \"true\"\n",
    "}\n",
    "\n",
    "df.write.format(\"hudi\") \\\n",
    "    .options(**hudi_options) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"s3a://warehouse/customer_hudi/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23605ac4-bd09-4614-88a0-2a6c202021e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-----------+\n",
      "|namespace|    tableName|isTemporary|\n",
      "+---------+-------------+-----------+\n",
      "|  default|customer_hudi|      false|\n",
      "+---------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES IN default\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6baf07e3-1820-4e96-91d3-311d2c7076dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# WARNING: Unable to get Instrumentation. Dynamic Attach failed. You may add this JAR as -javaagent manually, or supply -Djdk.attach.allowAttachSelf\n",
      "# WARNING: Unable to attach Serviceability Agent. Unable to attach even with module exceptions: [org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+-----+---+--------------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| id| name|age|      curr_timestamp|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+-----+---+--------------------+\n",
      "|  20251027161210964|20251027161210964...|                 2|                      |2b6da6f0-fba9-4a3...|  2|  Bob| 30|2025-10-27 16:12:...|\n",
      "|  20251027161210964|20251027161210964...|                 1|                      |2b6da6f0-fba9-4a3...|  1|Alice| 24|2025-10-27 16:12:...|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+-----+---+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM default.customer_hudi\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a740029f-b034-4be1-b514-feea64aef97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW SCHEMAS\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd206f6-4d21-4f78-a2bf-8e299b71c4c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09813ea9-8209-4f67-8652-ad55509fff2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/28 14:46:46 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "25/10/28 14:46:46 INFO SparkUI: Stopped Spark web UI at http://9361fa1ccb18:4040\n",
      "25/10/28 14:46:46 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "25/10/28 14:46:46 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down\n",
      "25/10/28 14:46:46 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "25/10/28 14:46:46 INFO MemoryStore: MemoryStore cleared\n",
      "25/10/28 14:46:46 INFO BlockManager: BlockManager stopped\n",
      "25/10/28 14:46:46 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "25/10/28 14:46:46 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "25/10/28 14:46:46 INFO SparkContext: Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c80267-4b5f-4b90-b21d-da7e0ae5f36a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
